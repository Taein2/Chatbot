{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n0EXgZ8NVeG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
      ],
      "metadata": {
        "id": "OZIG5INiNW80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/drive/MyDrive/Colab Notebooks/챗봇/ChatbotData.csv'\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()"
      ],
      "metadata": {
        "id": "BqsFLTltOCdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
        "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs,labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "\n",
        "train_size = int(len(padded_seqs) * 0.7)\n",
        "val_size = int(len(padded_seqs) * 0.2)\n",
        "test_size = int(len(padded_seqs) * 0.1)\n",
        "\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
        "\n",
        "# 하이퍼 파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 5\n",
        "VOCAB_SIZE = len(word_index) + 1 # 전체 단어 수\n",
        "\n",
        "# CNN 모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN, ))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "\n",
        "conv1 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=3,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "\n",
        "conv2 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=4,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "\n",
        "conv3 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=5,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)\n",
        "\n",
        "# 3, 4, 5-gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "logits = Dense(3, name='logits')(dropout_hidden)\n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
        "\n",
        "# 모델 생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
        "\n",
        "# 모델 평가(테스트 데이터셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy : %f' % (accuracy * 100))\n",
        "print('loss : %f' % (loss))\n",
        "\n",
        "# 모델 저장\n",
        "model.save('cnn_model.h5')"
      ],
      "metadata": {
        "id": "uFOBEZH4OOaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model, load_model"
      ],
      "metadata": {
        "id": "rLn_eYuGPgtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/drive/MyDrive/Colab Notebooks/챗봇/ChatbotData.csv'\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "# 테스트용 데이터셋 생성\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs,labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "test_ds = ds.take(2000).batch(20) # 테스트 데이터셋\n",
        "\n",
        "# 감정 분류 CNN 모델 불러오기\n",
        "model = load_model('cnn_model.h5')\n",
        "model.summary()\n",
        "model.evaluate(test_ds, verbose=2)\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 출력\n",
        "print(\"단어 시퀀스 : \", corpus[10212])\n",
        "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])\n",
        "print(\"문장 분류(정답) : \", labels[10212])\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
        "picks = [10212]\n",
        "predict = model.predict(padded_seqs[picks])\n",
        "predict_class = tf.math.argmax(predict, axis=1)\n",
        "print(\"감정 예측 점수 : \", predict)\n",
        "print(\"감정 예측 클래스 : \", predict_class.numpy())"
      ],
      "metadata": {
        "id": "hRDPmXWkSWFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}